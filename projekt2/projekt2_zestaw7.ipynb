{"cells": [{"cell_type": "markdown", "id": "861b6d4a-0de6-42ba-97a5-beef1f82f292", "metadata": {}, "source": "# Projekt Apache Spark"}, {"cell_type": "markdown", "id": "7b301ae8-ceff-4dbf-8d04-75bb4eb52480", "metadata": {}, "source": "# Wprowadzenie\n\nWykorzystuj\u0105c ten notatnik jako szablon zrealizuj projekt Apache Spark zgodnie z przydzielonym zestawem. \n\nKilka uwag:\n\n* Nie modyfikuj ani nie usuwaj paragraf\u00f3w *markdown* w tym notatniku, chyba \u017ce wynika to jednoznacznie z instrukcji. \n* Istniej\u0105ce paragrafy zawieraj\u0105ce *kod* uzupe\u0142nij w razie potrzeby zgodnie z instrukcjami\n    - nie usuwaj ich\n    - nie usuwaj zawartych w nich instrukcji oraz kodu\n    - nie modyfikuj ich, je\u015bli instrukcje jawnie tego nie nakazuj\u0105\n* Mo\u017cesz dodawa\u0107 nowe paragrafy zar\u00f3wno zawieraj\u0105ce kod jak i komentarze dotycz\u0105ce tego kodu (markdown)"}, {"cell_type": "markdown", "id": "e69d12f1-1013-4c74-b6aa-686ccfcbdd5c", "metadata": {}, "source": "# Tre\u015b\u0107 projektu\n\nPoni\u017cej w paragrafie markdown wstaw tytu\u0142 przydzielonego zestawu"}, {"cell_type": "markdown", "id": "adfc4ff6-4d43-49ed-a0d1-8b6988eaec16", "metadata": {}, "source": "# Zestaw 7 \u2013 carstrucks-data\n\n**Uwaga**\n\n- W ramach wzorca nie s\u0105 spe\u0142nione \u017cadne regu\u0142y projektu. \n- Brak konsekwencji w wykorzystaniu w\u0142a\u015bciwego API w ramach poszczeg\u00f3lnych cz\u0119\u015bci\n- Zadanie *misji g\u0142\u00f3wnej* polega na zliczeniu s\u0142\u00f3wek.  "}, {"cell_type": "markdown", "id": "5e128e43-6cce-4ffa-9609-9fae4b164ae9", "metadata": {}, "source": "# Dzia\u0142ania wst\u0119pne \n\nUruchom poni\u017cszy paragraf, aby utworzy\u0107 obiekty kontekstu Sparka. Je\u015bli jest taka potrzeba dostosuj te polecenia. Pami\u0119taj po potrzebnych bibliotekach."}, {"cell_type": "code", "execution_count": 1, "id": "26fb1050-386f-4398-ba5a-b45f5065d87b", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n24/01/11 20:43:51 INFO SparkEnv: Registering MapOutputTracker\n24/01/11 20:43:51 INFO SparkEnv: Registering BlockManagerMaster\n24/01/11 20:43:51 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n24/01/11 20:43:51 INFO SparkEnv: Registering OutputCommitCoordinator\n"}], "source": "from pyspark.sql import SparkSession\n\n# Spark session & context\nspark = SparkSession.builder.getOrCreate()\n\nsc = spark.sparkContext"}, {"cell_type": "markdown", "id": "8695a354-52bc-4bba-8222-7121bf07ae90", "metadata": {}, "source": "W poni\u017cszym paragrafie uzupe\u0142nij polecenia definiuj\u0105ce poszczeg\u00f3lne zmienne. \n\nPami\u0119taj aby\u015b:\n\n* w p\u00f3\u017aniejszym kodzie, dla wszystkich cze\u015bci projektu, korzysta\u0142 z tych zdefiniowanych zmiennych. Wykorzystuj je analogicznie jak parametry\n* przed ostateczn\u0105 rejestracj\u0105 projektu usun\u0105\u0142 ich warto\u015bci, tak aby nie pozostawia\u0107 w notatniku niczego co mog\u0142oby identyfikowa\u0107 Ciebie jako jego autora"}, {"cell_type": "code", "execution_count": 2, "id": "e883af01-7117-4faa-a840-7ff807a195d9", "metadata": {}, "outputs": [], "source": "# pe\u0142na \u015bcie\u017cka do katalogu w zasobniku zawieraj\u0105cego podkatalogi `datasource1` i `datasource4` \n# z danymi \u017ar\u00f3d\u0142owymi\ninput_dir = \"gs://miktar/zestaw7/input\""}, {"cell_type": "markdown", "id": "4601cc7a-3ed5-47e2-994f-ebec642049b5", "metadata": {}, "source": "Nie modyfikuj poni\u017cszych paragraf\u00f3w. Wykonaj je i u\u017cywaj zdefniowanych poni\u017cej zmiennych jak parametr\u00f3w Twojego programu."}, {"cell_type": "code", "execution_count": 3, "id": "6167e297-01ed-463e-bb81-9104d7cf7093", "metadata": {}, "outputs": [], "source": "# NIE ZMIENIA\u0106\n# \u015bcie\u017cki dla danych \u017ar\u00f3d\u0142owych \ndatasource1_dir = input_dir + \"/datasource1\"\ndatasource4_dir = input_dir + \"/datasource4\"\n\n# nazwy i \u015bcie\u017cki dla wynik\u00f3w dla misji g\u0142\u00f3wnej \n# cz\u0119\u015b\u0107 1 (Spark Core - RDD) \nrdd_result_dir = \"/tmp/output1\"\n\n# cz\u0119\u015b\u0107 2 (Spark SQL - DataFrame)\ndf_result_table = \"output2\"\n\n# cz\u0119\u015b\u0107 3 (Pandas API on Spark)\nps_result_file = \"/tmp/output3.json\""}, {"cell_type": "code", "execution_count": 4, "id": "e36e0314-a4ac-4096-9e4b-23fd4a73e0a9", "metadata": {}, "outputs": [], "source": "# NIE ZMIENIA\u0106\nimport os\ndef remove_file(file):\n    if os.path.exists(file):\n        os.remove(file)\n\nremove_file(\"metric_functions.py\")\nremove_file(\"tools_functions.py\")"}, {"cell_type": "code", "execution_count": 5, "id": "1b4b8e00-10ae-47dc-b623-d1dacbe9c86b", "metadata": {}, "outputs": [{"data": {"text/plain": "3322"}, "execution_count": 5, "metadata": {}, "output_type": "execute_result"}], "source": "# NIE ZMIENIA\u0106\nimport requests\nr = requests.get(\"https://jankiewicz.pl/bigdata/metric_functions.py\", allow_redirects=True)\nopen('metric_functions.py', 'wb').write(r.content)\nr = requests.get(\"https://jankiewicz.pl/bigdata/tools_functions.py\", allow_redirects=True)\nopen('tools_functions.py', 'wb').write(r.content)"}, {"cell_type": "code", "execution_count": 6, "id": "0a433894-dc97-46f2-be51-9f40fa36894f", "metadata": {}, "outputs": [], "source": "# NIE ZMIENIA\u0106\n%run metric_functions.py\n%run tools_functions.py"}, {"cell_type": "markdown", "id": "c9d3a9dc-ac3b-4316-abb9-365caa1d7185", "metadata": {}, "source": "Poni\u017csze paragrafy maj\u0105 na celu usun\u0105\u0107 ewentualne pozosta\u0142o\u015bci poprzednich uruchomie\u0144 tego lub innych notatnik\u00f3w"}, {"cell_type": "code", "execution_count": 7, "id": "08091c72-937f-41c2-9afe-d1505862bf1c", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "rm: `/tmp/output1': No such file or directory\n"}, {"name": "stdout", "output_type": "stream", "text": "Error deleting file /tmp/output1: Command '['hadoop', 'fs', '-rm', '-r', '/tmp/output1']' returned non-zero exit status 1.\n"}], "source": "# NIE ZMIENIA\u0106\n# usuni\u0119cie miejsca docelowego dla cz\u0119\u015b\u0107 1 (Spark Core - RDD) \ndelete_dir(spark, rdd_result_dir)"}, {"cell_type": "code", "execution_count": 8, "id": "f3e863c0-c824-47bd-b53a-ce3b1fd6d453", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "ivysettings.xml file not found in HIVE_HOME or HIVE_CONF_DIR,/etc/hive/conf.dist/ivysettings.xml will be used\n"}, {"name": "stdout", "output_type": "stream", "text": "The table output2 does not exist.\nError deleting file file:/spark-warehouse/output2: Command '['hadoop', 'fs', '-rm', '-r', 'file:/spark-warehouse/output2']' returned non-zero exit status 1.\n"}, {"name": "stderr", "output_type": "stream", "text": "rm: `file:/spark-warehouse/output2': No such file or directory\n"}], "source": "# NIE ZMIENIA\u0106\n# usuni\u0119cie miejsca docelowego dla cz\u0119\u015b\u0107 2 (Spark SQL - DataFrame) \ndrop_table(spark, df_result_table)"}, {"cell_type": "code", "execution_count": 9, "id": "72956a1a-da48-4d2b-a07a-e03d56431d6e", "metadata": {}, "outputs": [], "source": "# NIE ZMIENIA\u0106\n# usuni\u0119cie miejsca docelowego dla cz\u0119\u015b\u0107 3 (Pandas API on Spark) \nremove_file(ps_result_file)"}, {"cell_type": "code", "execution_count": 10, "id": "b9e423d4-92b8-4161-98da-1a867f86d780", "metadata": {}, "outputs": [{"data": {"text/html": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://pbd-cluster-m.europe-central2-c.c.big-data-2023-10-mt.internal:37917\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>yarn</code></dd>\n              <dt>AppName</dt>\n                <dd><code>pyspark-shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ", "text/plain": "<pyspark.sql.session.SparkSession at 0x7f66412b7a30>"}, "execution_count": 10, "metadata": {}, "output_type": "execute_result"}], "source": "# NIE ZMIENIA\u0106\nspark"}, {"cell_type": "markdown", "id": "14faf05b-6c52-4b02-b2e5-2ddb3f38c704", "metadata": {}, "source": "***Uwaga!***\n\nUruchom poni\u017cszy paragraf i sprawd\u017a czy adres, pod kt\u00f3rym dost\u0119pny *Apache Spark Application UI* jest poprawny wywo\u0142uj\u0105c nast\u0119pny testowy paragraf. \n\nW razie potrzeby okre\u015bl samodzielnie poprawny adres, pod kt\u00f3rym dost\u0119pny *Apache Spark Application UI*"}, {"cell_type": "code", "execution_count": 11, "id": "32acf3d2-ec4e-469d-bb0b-5f260c2c8e3b", "metadata": {}, "outputs": [{"data": {"text/plain": "'http://pbd-cluster-m.europe-central2-c.c.big-data-2023-10-mt.internal:37917'"}, "execution_count": 11, "metadata": {}, "output_type": "execute_result"}], "source": "# adres URL, pod kt\u00f3rym dost\u0119pny Apache Spark Application UI (REST API)\n# \nspark_ui_address = extract_host_and_port(spark, \"http://localhost:4041\")\nspark_ui_address"}, {"cell_type": "code", "execution_count": 12, "id": "32c2329e-1d7a-465f-a23b-333f95bf7deb", "metadata": {}, "outputs": [{"data": {"text/plain": "{'numTasks': 0,\n 'numActiveTasks': 0,\n 'numCompleteTasks': 0,\n 'numFailedTasks': 0,\n 'numKilledTasks': 0,\n 'numCompletedIndices': 0,\n 'executorDeserializeTime': 0,\n 'executorDeserializeCpuTime': 0,\n 'executorRunTime': 0,\n 'executorCpuTime': 0,\n 'resultSize': 0,\n 'jvmGcTime': 0,\n 'resultSerializationTime': 0,\n 'memoryBytesSpilled': 0,\n 'diskBytesSpilled': 0,\n 'peakExecutionMemory': 0,\n 'inputBytes': 0,\n 'inputRecords': 0,\n 'outputBytes': 0,\n 'outputRecords': 0,\n 'shuffleRemoteBlocksFetched': 0,\n 'shuffleLocalBlocksFetched': 0,\n 'shuffleFetchWaitTime': 0,\n 'shuffleRemoteBytesRead': 0,\n 'shuffleRemoteBytesReadToDisk': 0,\n 'shuffleLocalBytesRead': 0,\n 'shuffleReadBytes': 0,\n 'shuffleReadRecords': 0,\n 'shuffleWriteBytes': 0,\n 'shuffleWriteTime': 0,\n 'shuffleWriteRecords': 0}"}, "execution_count": 12, "metadata": {}, "output_type": "execute_result"}], "source": "# testowy paragraf\ntest_metrics = get_current_metrics(spark_ui_address)\ntest_metrics"}, {"cell_type": "markdown", "id": "f5ccca69-c577-440c-aa5c-c9df3a54e127", "metadata": {}, "source": "# Cz\u0119\u015b\u0107 1 - Spark Core (RDD)\n\n## Misje poboczne\n\nW ponizszych paragrafach wprowad\u017a swoje rozwi\u0105zania *misji pobocznych*, o ile **nie** chcesz, aby oceniana by\u0142a *misja g\u0142\u00f3wna*. W przeciwnym przypadku **KONIECZNIE** pozostaw je **puste**.  "}, {"cell_type": "code", "execution_count": 13, "id": "f0af3440-983a-4cac-a8e7-4908b010947c", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 1:=======================================================>(99 + 1) / 100]\r"}, {"name": "stdout", "output_type": "stream", "text": "+------------+------+-------------+-------------------+\n|transmission|  cars|    price_max|       diesel_ratio|\n+------------+------+-------------+-------------------+\n|   automatic|315790|3.736928711E9|0.07107888153519744|\n|       other| 59687|     1.0004E7|0.01960225844823831|\n|      manual| 21902|1.410065407E9|0.06940005478951694|\n|        null|  1495|     449500.0|0.06555183946488294|\n+------------+------+-------------+-------------------+\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import max as spark_max, sum as spark_sum, col\n\n# Inicjalizacja Spark Session\nspark = SparkSession.builder.getOrCreate()\n\n# Wczytywanie danych\ndatasource1_rdd = spark.sparkContext.textFile(datasource1_dir).map(lambda line: line.split(\"^\"))\n\n# Funkcja do filtrowania rekord\u00f3w\ndef is_valid(record):\n    empty = ['', 'null']\n    return sum(x in empty for x in [record[9], record[6], record[0], record[2]]) < 2\n\n# Filtracja rekord\u00f3w\nfiltered_records = datasource1_rdd.filter(is_valid)\n\n# Mapowanie danych\ndef map_record(record):\n    transmission = record[9]\n    price = float(record[0]) if record[0] != '' else 0.0\n    is_diesel = 1 if 'diesel' in record[6].lower() else 0\n    return (transmission, 1, price, is_diesel)\n\nmapped_records = filtered_records.map(map_record)\n\n# Tworzenie DataFrame\nschema = [\"transmission\", \"count\", \"price\", \"is_diesel\"]\ndf = spark.createDataFrame(mapped_records, schema=schema)\n\n# Agregacje\nresult = df.groupBy(\"transmission\").agg(\n    spark_sum(\"count\").alias(\"cars\"),\n    spark_max(\"price\").alias(\"price_max\"),\n    (spark_sum(\"is_diesel\") / spark_sum(\"count\")).alias(\"diesel_ratio\")\n)\n\n# Wy\u015bwietlenie wyniku\nresult.show()"}, {"cell_type": "code", "execution_count": 14, "id": "5fc37879-e0fa-4c4a-bd0d-4c01c3ecf38a", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "DataFrame[transmission: string, cars: bigint, price_max: double, diesel_ratio: double]\n"}], "source": "# Zapisywanie wyniku\nprint(result)"}, {"cell_type": "markdown", "id": "d303a72b-4083-470e-b25d-3224360ee94f", "metadata": {}, "source": "## Misja g\u0142\u00f3wna \n\nPoni\u017cszy paragraf zapisuje metryki przed uruchomieniem Twojego rozwi\u0105zania *misji g\u0142\u00f3wnej*. \n\nNie musisz go uruchamia\u0107 podczas implementacji rozwi\u0105zania."}, {"cell_type": "code", "execution_count": 13, "id": "037689d7-f0ee-4165-bef0-83fa7f3e8346", "metadata": {}, "outputs": [], "source": "# NIE ZMIENIA\u0106\nbefore_rdd_metrics = get_current_metrics(spark_ui_address)"}, {"cell_type": "markdown", "id": "b23971c0-cec7-4ea8-befb-7f063dce863c", "metadata": {}, "source": "W poni\u017cszych paragrafach wprowad\u017a **rozwi\u0105zanie** *misji g\u0142\u00f3wnej* oparte na *RDD API*. \n\nPami\u0119taj o wydajno\u015bci Twojego przetwarzania, *RDD API* tego wymaga. \n\nNie wprowadzaj w poni\u017cszych paragrafach \u017cadnego kodu, w przypadku wykorzystania *misji pobocznych*."}, {"cell_type": "code", "execution_count": 14, "id": "8af00c41-02a9-4a85-b3c6-bc41098edbe2", "metadata": {}, "outputs": [], "source": "# Wczytanie plik\u00f3w tekstowych\ntext_files = sc.textFile(datasource4_dir)"}, {"cell_type": "code", "execution_count": 15, "id": "d7955a5f-386d-47a5-9f6a-3d93a906c526", "metadata": {}, "outputs": [], "source": "# Podzia\u0142 linii na s\u0142owa i zliczanie ilo\u015bci wyst\u0105pie\u0144 ka\u017cdego s\u0142owa\nword_counts = text_files.flatMap(lambda line: line.split(\" \")) \\\n                        .map(lambda word: (word, 1)) \\\n                        .reduceByKey(lambda x, y: x + y)"}, {"cell_type": "code", "execution_count": 16, "id": "91d77fd7-1f15-4365-ae80-c902aeb55ce7", "metadata": {}, "outputs": [], "source": "# Zapis wyniku do pliku pickle\nword_counts.saveAsPickleFile(rdd_result_dir)"}, {"cell_type": "markdown", "id": "42d8b5ec-b799-4177-8e4a-80a583d995e7", "metadata": {}, "source": "Poni\u017cszy paragraf zapisuje metryki po uruchomieniu Twojego rozwi\u0105zania *misji g\u0142\u00f3wnej*. \n\nNie musisz go uruchamia\u0107 podczas implementacji rozwi\u0105zania."}, {"cell_type": "code", "execution_count": 17, "id": "4325d378-b145-4e8f-8d37-80a072b506c3", "metadata": {}, "outputs": [], "source": "# NIE ZMIENIA\u0106\nafter_rdd_metrics = get_current_metrics(spark_ui_address)"}, {"cell_type": "markdown", "id": "28137d3d-6f0d-443f-97b8-38104aaced6d", "metadata": {}, "source": "# Cz\u0119\u015b\u0107 2 - Spark SQL (DataFrame)\n\n## Misje poboczne\n\nW ponizszych paragrafach wprowad\u017a swoje rozwi\u0105zania *misji pobocznych*, o ile **nie** chcesz, aby oceniana by\u0142a *misja g\u0142\u00f3wna*. W przeciwnym przypadku **KONIECZNIE** pozostaw je **puste**.  "}, {"cell_type": "code", "execution_count": 15, "id": "6d045dae-5826-4015-8833-564d356db1f8", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "[Stage 15:>                                                         (0 + 1) / 1]\r"}, {"name": "stdout", "output_type": "stream", "text": "+-----+-------+------------------+-----+\n|state|regions|         price_avg| cars|\n+-----+-------+------------------+-----+\n|   AZ|      6| 17134.18861627771| 2872|\n|   SC|      5| 17413.34270491705| 3132|\n|   LA|      7| 255905.0492746099| 7938|\n|   MN|      8| 91232.40060769112| 4661|\n|   NJ|      3| 14863.94321686863| 2484|\n|   OR|      8|504028.88828232384| 6799|\n|   VA|     11|20914.347262598032| 5719|\n|   RI|      1|10586.887640449439|   89|\n|   KY|      6|22215.166670735485| 2172|\n|   WY|      1|  25863.4368673415| 1857|\n|   NH|      1|           12750.0|    2|\n|   MI|     16| 16662.06547562716|14740|\n|   NV|      3|18671.910739280353| 4843|\n|   WI|     12|17752.862973888474| 9586|\n|   ID|      6|14704.943224438517| 7080|\n|   CA|     26|178431.47549267113|24398|\n|   CT|      3| 33154.17254619106|  967|\n|   NE|      4|14840.123964285714| 1446|\n|   MT|      6|14894.931763293096| 6227|\n|   NC|     12|15295.050914805122| 6190|\n+-----+-------+------------------+-----+\nonly showing top 20 rows\n\n"}, {"name": "stderr", "output_type": "stream", "text": "                                                                                \r"}], "source": "from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, avg, when, sum as spark_sum, countDistinct\n\nspark = SparkSession.builder.getOrCreate()\n\ndatasource1_df = spark.read.csv(datasource1_dir, sep=\"^\", inferSchema=True, header=False).toDF(\n    \"price\", \"year\", \"manufacturer\", \"model\", \"condition\", \"cylinders\", \"fuel\", \"odometer\", \n    \"title_status\", \"transmission\", \"VIN\", \"drive\", \"size\", \"type\", \"paint_color\", \n    \"image_url\", \"posting_date\", \"geo_id\"\n)\ndatasource4_df = spark.read.csv(datasource4_dir, sep=\"^\", inferSchema=True, header=False).toDF(\n    \"id\", \"region\", \"region_url\", \"county\", \"state\"\n)\n\nclean_automatic_cars = datasource1_df.filter((col(\"title_status\") == \"clean\") & (col(\"transmission\") == \"automatic\"))\n\njoined_df = clean_automatic_cars.join(datasource4_df, clean_automatic_cars.geo_id == datasource4_df.id)\ngrouped_df = joined_df.groupBy(\"state\", \"region\", \"geo_id\").agg(\n    avg(\"price\").alias(\"avg_price\"),\n    spark_sum(when(col(\"price\") > 0, 1).otherwise(0)).alias(\"car_count\")\n)\n\nautomatic_ratio_df = datasource1_df.groupBy(\"geo_id\").agg(\n    spark_sum(when(col(\"transmission\") == \"automatic\", 1).otherwise(0)).alias(\"automatic_count\"),\n    spark_sum(when(col(\"price\") > 0, 1).otherwise(0)).alias(\"total_count\")\n).withColumn(\"automatic_ratio\", col(\"automatic_count\") / col(\"total_count\"))\n\nregions_with_majority_automatic = automatic_ratio_df.filter(col(\"automatic_ratio\") > 0.5)\n\nfinal_join = regions_with_majority_automatic.join(grouped_df, \"geo_id\")\n\nfinal_result = final_join.groupBy(\"state\").agg(\n    countDistinct(\"region\").alias(\"regions\"),\n    avg(\"avg_price\").alias(\"price_avg\"),\n    spark_sum(\"car_count\").alias(\"cars\")\n)\n\nfinal_result.show()\n"}, {"cell_type": "code", "execution_count": null, "id": "f7738406-c426-4238-b0fb-983f4585bc5a", "metadata": {}, "outputs": [], "source": ""}, {"cell_type": "markdown", "id": "5e7e569f-5f6b-4a98-b177-1b6fb0fc3333", "metadata": {}, "source": "## Misja g\u0142\u00f3wna \n\nPoni\u017cszy paragraf zapisuje metryki przed uruchomieniem Twojego rozwi\u0105zania *misji g\u0142\u00f3wnej*. \n\nNie musisz go uruchamia\u0107 podczas implementacji rozwi\u0105zania."}, {"cell_type": "code", "execution_count": 18, "id": "6329c04b-3e50-41a8-93f1-333ac0ea64ce", "metadata": {}, "outputs": [], "source": "# NIE ZMIENIA\u0106\nbefore_df_metrics = get_current_metrics(spark_ui_address)"}, {"cell_type": "markdown", "id": "4c2cfb0d-51b6-45bb-b173-ab8ac630d4f3", "metadata": {}, "source": "W poni\u017cszych paragrafach wprowad\u017a **rozwi\u0105zanie** *misji g\u0142\u00f3wnej* swojego projektu oparte o *DataFrame API*. \n\nPami\u0119taj o wydajno\u015bci Twojego przetwarzania, *DataFrame API* nie jest w stanie wszystkiego \"naprawi\u0107\". \n\nNie wprowadzaj w poni\u017cszych paragrafach \u017cadnego kodu, w przypadku wykorzystania *misji pobocznych*."}, {"cell_type": "code", "execution_count": 19, "id": "eca6e627-0ce5-4c48-b441-3bcc14e32f36", "metadata": {}, "outputs": [], "source": "from pyspark.sql.functions import split, explode, count\n# Wczytanie danych\ndata = spark.read.text(datasource4_dir)"}, {"cell_type": "code", "execution_count": 20, "id": "bcc4aaa9-8dc2-4726-871e-5e2450ba3fa8", "metadata": {}, "outputs": [], "source": "# Dzielenie linii na s\u0142owa i eksplozja do osobnych wierszy\nwords = data.select(explode(split(data.value, \" \")).alias(\"word\"))\n\n# Zliczanie s\u0142\u00f3w\nword_counts = words.groupBy(\"word\").agg(count(\"word\").alias(\"count\"))"}, {"cell_type": "code", "execution_count": 21, "id": "45165cca-5197-4590-ba69-7541085147f9", "metadata": {}, "outputs": [], "source": "# Zapis wynik\u00f3w do tabeli \nword_counts.write.mode(\"overwrite\").saveAsTable(df_result_table)"}, {"cell_type": "markdown", "id": "d0797752-450e-4f8f-a1d4-93a890a62c3d", "metadata": {}, "source": "Poni\u017cszy paragraf zapisuje metryki po uruchomieniu Twojego rozwi\u0105zania *misji g\u0142\u00f3wnej*. \n\nNie musisz go uruchamia\u0107 podczas implementacji rozwi\u0105zania."}, {"cell_type": "code", "execution_count": 22, "id": "c3647eae-2801-46ac-b43d-74e5bbfcab52", "metadata": {}, "outputs": [], "source": "# NIE ZMIENIA\u0106\nafter_df_metrics = get_current_metrics(spark_ui_address)"}, {"cell_type": "markdown", "id": "3bed01aa-cc23-427e-84c8-e5b76b9323bb", "metadata": {}, "source": "# Cz\u0119\u015b\u0107 3 - Pandas API on Spark\n\nTa cz\u0119\u015b\u0107 to wyzwanie. W szczeg\u00f3lno\u015bci dla os\u00f3b, kt\u00f3re nie programuj\u0105 na co dzie\u0144 w Pythonie, lub kt\u00f3re nie nie korzysta\u0142y do tej pory z Pandas API.  \n\nPowodzenia!\n\n## Misje poboczne\n\nW ponizszych paragrafach wprowad\u017a swoje rozwi\u0105zania *misji pobocznych*, o ile **nie** chcesz, aby oceniana by\u0142a *misja g\u0142\u00f3wna*. W przeciwnym przypadku **KONIECZNIE** pozostaw je **puste**.  "}, {"cell_type": "code", "execution_count": 16, "id": "971a265f-db04-4a26-936d-18ab875ddffa", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "/usr/lib/spark/python/pyspark/pandas/__init__.py:49: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n  warnings.warn(\n/usr/lib/spark/python/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_csv`, the default index is attached which can cause additional overhead.\n  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n                                                                                \r"}, {"data": {"text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>price_max</th>\n      <th>diesel_ratio</th>\n      <th>count</th>\n    </tr>\n    <tr>\n      <th>transmission</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>automatic</th>\n      <td>3736928711</td>\n      <td>0.071079</td>\n      <td>315790</td>\n    </tr>\n    <tr>\n      <th>other</th>\n      <td>10004000</td>\n      <td>0.019602</td>\n      <td>59687</td>\n    </tr>\n    <tr>\n      <th>manual</th>\n      <td>1410065407</td>\n      <td>0.069400</td>\n      <td>21902</td>\n    </tr>\n    <tr>\n      <th>null</th>\n      <td>449500</td>\n      <td>0.046600</td>\n      <td>2103</td>\n    </tr>\n  </tbody>\n</table>\n</div>", "text/plain": "               price_max  diesel_ratio   count\ntransmission                                  \nautomatic     3736928711      0.071079  315790\nother           10004000      0.019602   59687\nmanual        1410065407      0.069400   21902\nnull              449500      0.046600    2103"}, "execution_count": 16, "metadata": {}, "output_type": "execute_result"}], "source": "import pyspark.pandas as ps\n\n# Ustawienie Pandas API on Spark\nps.set_option(\"compute.default_index_type\", \"distributed\")\n\n# Wczytywanie danych do Pandas DataFrame on Spark\ndatasource1_ps = ps.read_csv(datasource1_dir, sep=\"^\", header=None, names=[\n    \"price\", \"year\", \"manufacturer\", \"model\", \"condition\", \"cylinders\", \"fuel\", \"odometer\", \n    \"title_status\", \"transmission\", \"VIN\", \"drive\", \"size\", \"type\", \"paint_color\", \n    \"image_url\", \"posting_date\", \"geo_id\"\n])\n\n# Filtracja danych\nfiltered_records_ps = datasource1_ps.dropna(subset=[\"transmission\", \"fuel\", \"price\", \"manufacturer\"], thresh=2)\n\n# Dodanie kolumny is_diesel\nfiltered_records_ps['is_diesel'] = filtered_records_ps['fuel'].apply(lambda x: 1 if x == 'diesel' else 0)\n\n# Agregacja danych\nresult_ps = filtered_records_ps.groupby('transmission').agg({\n    'price': 'max',\n    'is_diesel': 'mean'\n}).rename(columns={'price': 'price_max', 'is_diesel': 'diesel_ratio'})\n\n# Liczba samochod\u00f3w dla ka\u017cdego typu transmisji\nfiltered_records_ps['count'] = 1\ncount_cars = filtered_records_ps.groupby('transmission')['count'].sum()\n\n# Po\u0142\u0105czenie wynik\u00f3w\nfinal_result_ps = result_ps.join(count_cars)\n\n# Wy\u015bwietlenie wynik\u00f3w\nfinal_result_ps.head()\n"}, {"cell_type": "code", "execution_count": 17, "id": "91621654-a24e-4ddb-b2c7-9f149252af13", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "/usr/lib/spark/python/pyspark/pandas/utils.py:975: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_csv`, the default index is attached which can cause additional overhead.\n  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n                                                                                \r"}, {"data": {"text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>regions</th>\n      <th>price_avg</th>\n      <th>cars</th>\n    </tr>\n    <tr>\n      <th>state</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>AZ</th>\n      <td>8</td>\n      <td>19114.042869</td>\n      <td>3091</td>\n    </tr>\n    <tr>\n      <th>SC</th>\n      <td>6</td>\n      <td>17108.385587</td>\n      <td>3830</td>\n    </tr>\n    <tr>\n      <th>LA</th>\n      <td>8</td>\n      <td>225854.010027</td>\n      <td>8812</td>\n    </tr>\n    <tr>\n      <th>MN</th>\n      <td>9</td>\n      <td>83455.372510</td>\n      <td>5117</td>\n    </tr>\n    <tr>\n      <th>NJ</th>\n      <td>4</td>\n      <td>14934.040746</td>\n      <td>2705</td>\n    </tr>\n  </tbody>\n</table>\n</div>", "text/plain": "       regions      price_avg  cars\nstate                              \nAZ           8   19114.042869  3091\nSC           6   17108.385587  3830\nLA           8  225854.010027  8812\nMN           9   83455.372510  5117\nNJ           4   14934.040746  2705"}, "execution_count": 17, "metadata": {}, "output_type": "execute_result"}], "source": "import pyspark.pandas as ps\n\nps.set_option(\"compute.ops_on_diff_frames\", True)\n\n# Wczytywanie danych\ndatasource1_ps = ps.read_csv(\n    datasource1_dir,\n    sep=\"^\",\n    header=None,\n    names=[\n        \"price\", \"year\", \"manufacturer\", \"model\", \"condition\", \"cylinders\",\n        \"fuel\", \"odometer\", \"title_status\", \"transmission\", \"VIN\", \"drive\",\n        \"size\", \"type\", \"paint_color\", \"image_url\", \"posting_date\", \"geo_id\"\n    ]\n)\ndatasource4_ps = ps.read_csv(\n    datasource4_dir,\n    sep=\"^\",\n    header=None,\n    names=[\"geo_id\", \"region\", \"region_url\", \"county\", \"state\"]\n)\n\n# Filtracja danych i przygotowanie\nclean_automatic_cars = datasource1_ps[\n    (datasource1_ps['title_status'] == 'clean') & \n    (datasource1_ps['transmission'] == 'automatic')\n]\n\n# Do\u0142\u0105czenie informacji o regionie i stanie\njoined_df = clean_automatic_cars.merge(datasource4_ps, on=\"geo_id\")\n\n# Dodanie kolumny okre\u015blaj\u0105cej automatyczn\u0105 skrzyni\u0119 bieg\u00f3w (jako 0 lub 1)\njoined_df['is_automatic'] = joined_df['transmission'].apply(lambda x: 1 if x == 'automatic' else 0)\n\n# Grupowanie danych i obliczanie stosunku\ngrouped_df = joined_df.groupby(['state', 'region']).agg({\n    'price': 'mean',\n    'geo_id': 'count',\n    'is_automatic': 'mean'\n}).rename(columns={'geo_id': 'car_count', 'is_automatic': 'automatic_ratio'})\n\nregions_with_majority_automatic = grouped_df.reset_index()\nregions_with_majority_automatic = regions_with_majority_automatic[regions_with_majority_automatic['automatic_ratio'] > 0.5]\n\n# Agregacja na poziomie stanu\nfinal_result = regions_with_majority_automatic.groupby('state').agg({\n    'region': 'count',\n    'price': 'mean',\n    'car_count': 'sum'\n}).rename(columns={'region': 'regions', 'price': 'price_avg', 'car_count': 'cars'})\n\n# Wy\u015bwietlenie wyniku\nfinal_result.head()\n"}, {"cell_type": "markdown", "id": "9a5184ce-cf42-4342-aeec-b56c30b66bbd", "metadata": {}, "source": "## Misja g\u0142\u00f3wna \n\nPoni\u017cszy paragraf zapisuje metryki przed uruchomieniem Twojego rozwi\u0105zania *misji g\u0142\u00f3wnej*. \n\nNie musisz go uruchamia\u0107 podczas implementacji rozwi\u0105zania."}, {"cell_type": "code", "execution_count": 23, "id": "63fd8306-87e9-46f2-b622-d60693e3ba6d", "metadata": {}, "outputs": [], "source": "#NIE ZMIENIA\u0106\nbefore_ps_metrics = get_current_metrics(spark_ui_address)"}, {"cell_type": "markdown", "id": "a967f079-7106-4bd7-9d26-98ced2aeb43b", "metadata": {}, "source": "W poni\u017cszych paragrafach wprowad\u017a **rozwi\u0105zanie** swojego projektu oparte o *Pandas API on Spark*. \n\nPami\u0119taj o wydajno\u015bci Twojego przetwarzania, *Pandas API on Spark* nie jest w stanie wszystkiego \"naprawi\u0107\". \n\nNie wprowadzaj w poni\u017cszych paragrafach \u017cadnego kodu, w przypadku wykorzystania *misji pobocznych*."}, {"cell_type": "code", "execution_count": 24, "id": "e2094a69-30b1-4970-825b-2b0624436cd5", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "/usr/local/spark/python/pyspark/pandas/__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n  warnings.warn(\n/usr/local/spark/python/pyspark/pandas/utils.py:1016: PandasAPIOnSparkAdviceWarning: If `index_col` is not specified for `read_csv`, the default index is attached which can cause additional overhead.\n  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"}], "source": "import pyspark.pandas as ps\n\nlines_ps = ps.read_csv(datasource4_dir, header=None)"}, {"cell_type": "code", "execution_count": 25, "id": "ea69e909-a557-4294-b1ae-f0d551649eec", "metadata": {}, "outputs": [], "source": "words_ps = lines_ps[0].apply(lambda x: x.split(' ') if x is not None else []).explode().reset_index(drop=True)"}, {"cell_type": "code", "execution_count": 26, "id": "b5759f50-b92e-41a5-9eb0-9b00e2528ce5", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "/usr/local/spark/python/pyspark/pandas/base.py:1437: FutureWarning: The resulting Series will have a fixed name of 'count' from 4.0.0.\n  warnings.warn(\n"}], "source": "word_counts = words_ps.value_counts()"}, {"cell_type": "code", "execution_count": 27, "id": "b02917f4-e1f2-4fb4-8b53-8829fb3f0689", "metadata": {}, "outputs": [{"name": "stderr", "output_type": "stream", "text": "/usr/local/spark/python/pyspark/pandas/utils.py:1016: PandasAPIOnSparkAdviceWarning: `to_pandas` loads all data into the driver's memory. It should only be used if the resulting pandas Series is expected to be small.\n  warnings.warn(message, PandasAPIOnSparkAdviceWarning)\n"}], "source": "word_counts_pandas = word_counts.head(50).to_pandas()"}, {"cell_type": "code", "execution_count": 28, "id": "76e0d7f7-82f3-41d4-8267-cf288f2f6e81", "metadata": {}, "outputs": [], "source": "word_counts_pandas.to_json(ps_result_file, orient='index')"}, {"cell_type": "markdown", "id": "298a0ec5-ab13-4e39-a572-e7adf8b8556a", "metadata": {}, "source": "Poni\u017cszy paragraf zapisuje metryki po uruchomieniu Twojego rozwi\u0105zania *misji g\u0142\u00f3wnej*. \n\nNie musisz go uruchamia\u0107 podczas implementacji rozwi\u0105zania."}, {"cell_type": "code", "execution_count": 30, "id": "108bee2a-a847-4625-8e4a-939951ac9201", "metadata": {}, "outputs": [], "source": "#NIE ZMIENIA\u0106\nafter_ps_metrics = get_current_metrics(spark_ui_address)"}, {"cell_type": "markdown", "id": "e32e266b-b5cd-41d0-aeab-c1edc365910d", "metadata": {}, "source": "# Analiza wynik\u00f3w i wydajno\u015bci *misji g\u0142\u00f3wnych*"}, {"cell_type": "markdown", "id": "46b67111-62d0-4657-b158-1ed37db9ed96", "metadata": {}, "source": "## Cz\u0119\u015b\u0107 1 - Spark Core (RDD)"}, {"cell_type": "code", "execution_count": 31, "id": "5cfc9900-7e0c-49ff-adba-e339f83ffe51", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "('TeriyakiApps\\x01~\\x01teriyakiapps@gmail.com\\x018589934594', 1)\n('Koza\\x01http://www.xynapse.pl\\x01xynapse@xynapse.pl\\x018589934595', 1)\n('Tools\\x01https://vcb30cb43.app-ads-txt.com/app-ads.txt\\x01androtools222@gmail.com\\x018589934596', 1)\n('Muslim', 110)\n('FireFlies', 2)\n('Studio\\x01~\\x01manuariza95@gmail.com\\x018589934602', 1)\n('News', 494)\n('IST-Development\\x01https://istanbulit.com\\x01info@istanbulit.com\\x018589934604', 1)\n('FAStuidoTI\\x01~\\x01karimkhalfy@gmail.com\\x018589934605', 1)\n('Web4Minds,', 1)\n('V3', 8)\n('Smart', 2437)\n('Ltd\\x01http://www.v3smarttech.com\\x01support@v3smarttech.com\\x018589934607', 1)\n('Mobil', 143)\n('UNDERSCORE:', 1)\n('Apps', 6350)\n('and', 5289)\n('Games\\x01~\\x01ergamesapps@gmail.com\\x018589934609', 1)\n('tamapps\\x01~\\x01zakdermeister@gmail.com\\x018589934614', 1)\n('S.', 397)\n('Connect', 331)\n('Team\\x01https://mewe.com/join/klwpdevelopersteam\\x01designcorpviti@gmail.com\\x018589934618', 1)\n('for', 2565)\n('with', 262)\n('NETWORKS', 23)\n('PTE', 226)\n('Art\\x01https://www.bytesart.site\\x01support@bytesart.tech\\x018589934624', 1)\n('Mother', 28)\n('ShowMeTheParts\\x01http://www.ShowMeTheParts.com\\x01showmetheparts@gmail.com\\x018589934628', 1)\n('Sitevenia\\x01http://www.wmphotos.fr\\x01williammoureaux@sfr.fr\\x018589934629', 1)\n('NOVATIVE\\x01https://www.novative.com/\\x01sales@novative.com\\x018589934634', 1)\n('Chokurei', 5)\n('everyone\\x01https://www.sistemaeducativofinanciero.com/p/privacy\\x01sanz112358@gmail.com\\x018589934635', 1)\n('Orotti', 1)\n('Apps\\x01http://www.orotti.com\\x01apps@orotti.com\\x018589934636', 1)\n('Rabbitz', 2)\n('Games\\x01~\\x01madrabbitzgames@gmail.com\\x018589934637', 1)\n('ParkerSoft\\x01~\\x01ianparker2007@yahoo.co.uk\\x018589934638', 1)\n('ISHAN', 5)\n('Bismania', 2)\n('Wei', 36)\n('Jie\\x01https://github.com/myluckynumbers/In-Between\\x01limweijie250@gmail.com\\x018589934644', 1)\n('Iraqi', 3)\n('Investment', 273)\n('MV', 59)\n('S/A\\x01http://www.mv.com.br\\x01inovacaomv@gmail.com\\x018589934646', 1)\n('Welfare', 43)\n('Gosa\\x01https://www.facebook.com/themexperia\\x01support@mkninc.ru\\x018589934648', 1)\n('Frillapps\\x01https://weedleapps.co.il/\\x01ozvi.inc@gmail.com\\x018589934651', 1)\n('Beansprites', 5)\n"}], "source": "# Wczytanie wynik\u00f3w z pliku pickle\nword_counts = sc.pickleFile(rdd_result_dir)\n\n# Wy\u015bwietlenie 50 pierwszych element\u00f3w\nresult_sample = word_counts.take(50)\nfor item in result_sample:\n    print(item)"}, {"cell_type": "code", "execution_count": 32, "id": "16edae69-8062-4422-842f-d50bca0af9a7", "metadata": {}, "outputs": [{"data": {"text/plain": "{'numTasks': 6,\n 'numActiveTasks': 0,\n 'numCompleteTasks': 6,\n 'numFailedTasks': 0,\n 'numKilledTasks': 0,\n 'numCompletedIndices': 6,\n 'executorDeserializeTime': 763,\n 'executorDeserializeCpuTime': 288417800,\n 'executorRunTime': 52789,\n 'executorCpuTime': 3791290300,\n 'resultSize': 12143,\n 'jvmGcTime': 1808,\n 'resultSerializationTime': 19,\n 'memoryBytesSpilled': 0,\n 'diskBytesSpilled': 0,\n 'peakExecutionMemory': 0,\n 'inputBytes': 84276905,\n 'inputRecords': 1179547,\n 'outputBytes': 90624535,\n 'outputRecords': 14566,\n 'shuffleRemoteBlocksFetched': 0,\n 'shuffleLocalBlocksFetched': 9,\n 'shuffleFetchWaitTime': 0,\n 'shuffleRemoteBytesRead': 0,\n 'shuffleRemoteBytesReadToDisk': 0,\n 'shuffleLocalBytesRead': 49730906,\n 'shuffleReadBytes': 49730906,\n 'shuffleReadRecords': 228,\n 'shuffleWriteBytes': 49730906,\n 'shuffleWriteTime': 405851800,\n 'shuffleWriteRecords': 228}"}, "execution_count": 32, "metadata": {}, "output_type": "execute_result"}], "source": "subtract_metrics(after_rdd_metrics, before_rdd_metrics)"}, {"cell_type": "markdown", "id": "efc730f1-4b5e-4a68-8a86-11768918fcf4", "metadata": {}, "source": "## Cz\u0119\u015b\u0107 2 - Spark SQL (DataFrame)"}, {"cell_type": "code", "execution_count": 33, "id": "b950a09d-045e-4143-a3cf-8ecc7c73ac41", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "+-------------------------+-----+\n|                     word|count|\n+-------------------------+-----+\n|                      The| 9372|\n|                   Bidhee|    7|\n|                Solutions| 6041|\n|                   ArtAce|    2|\n|                  PuyTech|    1|\n|                   McLeod|  208|\n|                      RTV|   13|\n|     Software\u0001http://p...|    1|\n|\u7d2b\u834a\u96dc\u8a8c\u793e\u0001https://bau...|    1|\n|                  Bacilio|    2|\n|     Developer\u0001https:/...|    1|\n|     Software\u0001http://w...|    1|\n|                  Backend|   13|\n|\ud558\uc774\ud37c\ud38c\ud504\u0001~\u0001hyper.cho...|    1|\n|                    METRO|   21|\n|     ADBAND\u0001http://www...|    1|\n|                      Tcf|    1|\n|                      Pug|   12|\n|              Techologies|    4|\n|     Tourism\u0001https://t...|    1|\n|     Kinsale\u0001~\u0001gourmet...|    1|\n|     English\u0001https://w...|    1|\n|                    Darul|   10|\n|                       \ud83d\udcf1|    3|\n|                  Panipat|    2|\n|     Konyukhov\u0001http://...|    1|\n|                     Bold|   38|\n|     Developer\u0001http://...|    1|\n|     Advertising\u0001http:...|    1|\n|                 C\u00c1NTABRO|    2|\n|     Consultores\u0001http:...|    1|\n|                     Amit|  106|\n|     CONTACTS\u0001http://w...|    1|\n|                    Jimmy|   78|\n|     applications\u0001~\u0001ph...|    1|\n|     Rechts\u0001https://ww...|    1|\n|     KetchapPro\u0001~\u0001ketc...|    1|\n|                     GIDA|    9|\n|     dev\u0001~\u0001radsdev@mai...|    1|\n|     Dipre\u0001~\u0001diomaris0...|    1|\n|                   Games:|   41|\n|                Beautiful|   62|\n|                      Jio|   22|\n|                   Phenix|   13|\n|     Apps\u0001https://seqa...|    1|\n|                    Qulam|    2|\n|     Games\u0001http://tinm...|    1|\n|     RedPACT\u0001~\u0001nancyak...|    1|\n|                     Coin|   79|\n|                 Smartify|    5|\n+-------------------------+-----+\n"}], "source": "df = spark.table(df_result_table)\n\n# Wy\u015bwietlenie 50 pierwszych rekord\u00f3w\ndf.show(50)"}, {"cell_type": "code", "execution_count": 34, "id": "3f344ed9-94c1-4d79-b839-1839548d8c67", "metadata": {}, "outputs": [{"data": {"text/plain": "{'numTasks': 12,\n 'numActiveTasks': 0,\n 'numCompleteTasks': 8,\n 'numFailedTasks': 0,\n 'numKilledTasks': 0,\n 'numCompletedIndices': 8,\n 'executorDeserializeTime': 1254,\n 'executorDeserializeCpuTime': 446474600,\n 'executorRunTime': 54900,\n 'executorCpuTime': 22626614900,\n 'resultSize': 36185,\n 'jvmGcTime': 2428,\n 'resultSerializationTime': 110,\n 'memoryBytesSpilled': 0,\n 'diskBytesSpilled': 0,\n 'peakExecutionMemory': 440400752,\n 'inputBytes': 84344235,\n 'inputRecords': 1179547,\n 'outputBytes': 50321941,\n 'outputRecords': 1456441,\n 'shuffleRemoteBlocksFetched': 0,\n 'shuffleLocalBlocksFetched': 16,\n 'shuffleFetchWaitTime': 0,\n 'shuffleRemoteBytesRead': 0,\n 'shuffleRemoteBytesReadToDisk': 0,\n 'shuffleLocalBytesRead': 63406957,\n 'shuffleReadBytes': 63406957,\n 'shuffleReadRecords': 1622698,\n 'shuffleWriteBytes': 63406957,\n 'shuffleWriteTime': 817119700,\n 'shuffleWriteRecords': 1622698}"}, "execution_count": 34, "metadata": {}, "output_type": "execute_result"}], "source": "subtract_metrics(after_df_metrics, before_df_metrics)"}, {"cell_type": "markdown", "id": "f063b46c-579d-4775-ba3f-837708279ea2", "metadata": {}, "source": "## Cz\u0119\u015b\u0107 3 - Pandas API on Spark"}, {"cell_type": "code", "execution_count": 35, "id": "ab5e31a2-fd31-40ca-be7b-b20b13dc38a2", "metadata": {}, "outputs": [{"name": "stdout", "output_type": "stream", "text": "{\n  \"-\": 12048,\n  \"&\": 11714,\n  \"The\": 9329,\n  \"App\": 8068,\n  \"of\": 7933,\n  \"de\": 7396,\n  \"Technologies\": 7056,\n  \"Solutions\": 6880,\n  \"Software\": 6813,\n  \"Co.\": 6800,\n  \"Apps\": 6579,\n  \"Media\": 6462,\n  \"Digital\": 5836,\n  \"Pvt\": 5602,\n  \"Pvt.\": 5290,\n  \"Games\": 5044,\n  \"Technology\": 5009,\n  \"and\": 4989,\n  \"Mobile\": 4754,\n  \"Private\": 4196,\n  \"\": 3879,\n  \"Development\": 3868,\n  \"Group\": 3761,\n  \"IT\": 3665,\n  \"Services\": 3527,\n  \"Tech\": 3445,\n  \"Game\": 3333,\n  \"Bank\": 3127,\n  \"by\": 2831,\n  \"Systems\": 2778,\n  \"International\": 2580,\n  \"Global\": 2579,\n  \"Web\": 2563,\n  \"for\": 2536,\n  \"BH\": 2507,\n  \"Appswiz\": 2452,\n  \"Smart\": 2436,\n  \"Studio\": 2330,\n  \"Credit\": 2215,\n  \"Pty\": 2202,\n  \"Free\": 2186,\n  \"Business\": 1840,\n  \"Radio\": 1810,\n  \"New\": 1778,\n  \"Health\": 1710,\n  \"Company\": 1710,\n  \"Online\": 1629,\n  \"My\": 1539,\n  \"Church\": 1516,\n  \"Creative\": 1458\n}\n"}], "source": "import json\n\n# Odczytaj zawarto\u015b\u0107 pliku JSON\nwith open(ps_result_file, 'r') as file:\n    json_content = json.load(file)\n\n# Wy\u015bwietl zawarto\u015b\u0107\nprint(json.dumps(json_content, indent=2))"}, {"cell_type": "code", "execution_count": 36, "id": "32788c91-3f8e-4fb1-8afc-5eb00938e687", "metadata": {}, "outputs": [{"data": {"text/plain": "{'numTasks': 33,\n 'numActiveTasks': 0,\n 'numCompleteTasks': 25,\n 'numFailedTasks': 0,\n 'numKilledTasks': 0,\n 'numCompletedIndices': 25,\n 'executorDeserializeTime': 1838,\n 'executorDeserializeCpuTime': 440241100,\n 'executorRunTime': 166601,\n 'executorCpuTime': 55323279000,\n 'resultSize': 134363,\n 'jvmGcTime': 4753,\n 'resultSerializationTime': 123,\n 'memoryBytesSpilled': 0,\n 'diskBytesSpilled': 0,\n 'peakExecutionMemory': 427817888,\n 'inputBytes': 385819487,\n 'inputRecords': 5409845,\n 'outputBytes': 0,\n 'outputRecords': 0,\n 'shuffleRemoteBlocksFetched': 0,\n 'shuffleLocalBlocksFetched': 20,\n 'shuffleFetchWaitTime': 0,\n 'shuffleRemoteBytesRead': 0,\n 'shuffleRemoteBytesReadToDisk': 0,\n 'shuffleLocalBytesRead': 61239298,\n 'shuffleReadBytes': 61239298,\n 'shuffleReadRecords': 1573467,\n 'shuffleWriteBytes': 61239298,\n 'shuffleWriteTime': 1111152100,\n 'shuffleWriteRecords': 1573467}"}, "execution_count": 36, "metadata": {}, "output_type": "execute_result"}], "source": "subtract_metrics(after_ps_metrics, before_ps_metrics)"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.8"}}, "nbformat": 4, "nbformat_minor": 5}